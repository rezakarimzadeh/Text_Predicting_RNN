{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ferdosi_next_mesra.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P8rlEMgnbx7",
        "outputId": "8bc37f26-60bd-493e-e651-4eb7f7a704fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFgV5Rykwqg7"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "\n",
        "     \n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "f=open('/content/drive/My Drive/ferdosi.txt',encoding='utf-8')\n",
        "first_part=[]\n",
        "second_part=[]\n",
        "max_first_part=0\n",
        "max_second_part=0\n",
        "chars=set()\n",
        "for lines in f:\n",
        "    lines=lines.strip()\n",
        "    x,y=lines.split(',')\n",
        "    input_texts.append(x)\n",
        "    target_texts.append(y)\n",
        "    max_first_part=max(max_first_part,len(x))\n",
        "    max_second_part=max(max_second_part,len(y))\n",
        "    for ch in lines:\n",
        "        input_characters.add(ch)\n",
        "f.close() \n",
        "input_characters=list(input_characters)\n",
        "input_characters.remove(',') \n",
        "PAD='_PAD_'\n",
        "BOM='_BOM_'\n",
        "EOM='_EOM_'\n",
        "input_characters=[BOM,PAD,EOM]+input_characters \n",
        "target_characters=input_characters\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DtB24g0wslD"
      },
      "source": [
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length= max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#for use _EOM_ & _BOM_\n",
        "max_encoder_seq_length+=2 \n",
        "max_decoder_seq_length+=2\n",
        "\n",
        "#make our dict\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "\n",
        "\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    encoder_input_data[i, :max_encoder_seq_length-len(input_text)-1, input_token_index['_PAD_']] = 1.\n",
        "    encoder_input_data[i, max_encoder_seq_length-len(input_text)-2, input_token_index['_BOM_']] = 1.\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, max_encoder_seq_length-len(input_text)-1+t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, max_encoder_seq_length-1, input_token_index['_EOM_']] = 1.\n",
        "    \n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t-1 , input_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t+1, input_token_index['_EOM_']] = 1.\n",
        "    decoder_input_data[i, t+2:, input_token_index['_PAD_']] = 1.\n",
        "    decoder_target_data[i, t, input_token_index['_EOM_']] = 1.\n",
        "    decoder_target_data[i, t+1:, input_token_index['_PAD_']] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN3tYUkznccI",
        "outputId": "e839d1c7-5111-4896-e93c-687f84346521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39687 samples, validate on 9922 samples\n",
            "Epoch 1/100\n",
            "39687/39687 [==============================] - 75s 2ms/step - loss: 1.6731 - acc: 0.5191 - val_loss: 1.4510 - val_acc: 0.5679\n",
            "Epoch 2/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 1.3121 - acc: 0.6104 - val_loss: 1.2933 - val_acc: 0.6161\n",
            "Epoch 3/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 1.1916 - acc: 0.6453 - val_loss: 1.2103 - val_acc: 0.6386\n",
            "Epoch 4/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 1.1171 - acc: 0.6671 - val_loss: 1.1493 - val_acc: 0.6619\n",
            "Epoch 5/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 1.0644 - acc: 0.6817 - val_loss: 1.1141 - val_acc: 0.6717\n",
            "Epoch 6/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 1.0256 - acc: 0.6927 - val_loss: 1.0916 - val_acc: 0.6769\n",
            "Epoch 7/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.9948 - acc: 0.7013 - val_loss: 1.0655 - val_acc: 0.6850\n",
            "Epoch 8/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.9635 - acc: 0.7102 - val_loss: 1.0376 - val_acc: 0.6960\n",
            "Epoch 9/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.9370 - acc: 0.7178 - val_loss: 1.0277 - val_acc: 0.6948\n",
            "Epoch 10/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.9157 - acc: 0.7240 - val_loss: 1.0088 - val_acc: 0.7010\n",
            "Epoch 11/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8973 - acc: 0.7293 - val_loss: 0.9985 - val_acc: 0.7054\n",
            "Epoch 12/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8807 - acc: 0.7341 - val_loss: 0.9899 - val_acc: 0.7077\n",
            "Epoch 13/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 0.8658 - acc: 0.7384 - val_loss: 0.9858 - val_acc: 0.7095\n",
            "Epoch 14/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8517 - acc: 0.7425 - val_loss: 0.9746 - val_acc: 0.7129\n",
            "Epoch 15/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8394 - acc: 0.7460 - val_loss: 0.9756 - val_acc: 0.7145\n",
            "Epoch 16/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8280 - acc: 0.7493 - val_loss: 0.9712 - val_acc: 0.7150\n",
            "Epoch 17/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8177 - acc: 0.7520 - val_loss: 0.9585 - val_acc: 0.7192\n",
            "Epoch 18/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.8080 - acc: 0.7547 - val_loss: 0.9669 - val_acc: 0.7175\n",
            "Epoch 19/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.7990 - acc: 0.7574 - val_loss: 0.9649 - val_acc: 0.7180\n",
            "Epoch 20/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.7906 - acc: 0.7601 - val_loss: 0.9546 - val_acc: 0.7224\n",
            "Epoch 21/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.7825 - acc: 0.7622 - val_loss: 0.9563 - val_acc: 0.7243\n",
            "Epoch 22/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.7750 - acc: 0.7641 - val_loss: 0.9563 - val_acc: 0.7221\n",
            "Epoch 23/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7678 - acc: 0.7664 - val_loss: 0.9518 - val_acc: 0.7239\n",
            "Epoch 24/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7609 - acc: 0.7682 - val_loss: 0.9569 - val_acc: 0.7242\n",
            "Epoch 25/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7544 - acc: 0.7703 - val_loss: 0.9616 - val_acc: 0.7222\n",
            "Epoch 26/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7483 - acc: 0.7720 - val_loss: 0.9621 - val_acc: 0.7221\n",
            "Epoch 27/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7421 - acc: 0.7738 - val_loss: 0.9642 - val_acc: 0.7230\n",
            "Epoch 28/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7366 - acc: 0.7754 - val_loss: 0.9593 - val_acc: 0.7242\n",
            "Epoch 29/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.7307 - acc: 0.7771 - val_loss: 0.9675 - val_acc: 0.7219\n",
            "Epoch 30/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7252 - acc: 0.7790 - val_loss: 0.9690 - val_acc: 0.7236\n",
            "Epoch 31/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7199 - acc: 0.7803 - val_loss: 0.9740 - val_acc: 0.7247\n",
            "Epoch 32/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7148 - acc: 0.7819 - val_loss: 0.9801 - val_acc: 0.7231\n",
            "Epoch 33/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7096 - acc: 0.7836 - val_loss: 0.9719 - val_acc: 0.7246\n",
            "Epoch 34/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.7050 - acc: 0.7846 - val_loss: 0.9766 - val_acc: 0.7239\n",
            "Epoch 35/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.7001 - acc: 0.7862 - val_loss: 0.9853 - val_acc: 0.7228\n",
            "Epoch 36/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6952 - acc: 0.7877 - val_loss: 0.9835 - val_acc: 0.7227\n",
            "Epoch 37/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6908 - acc: 0.7889 - val_loss: 0.9903 - val_acc: 0.7224\n",
            "Epoch 38/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.6862 - acc: 0.7902 - val_loss: 0.9855 - val_acc: 0.7234\n",
            "Epoch 39/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.6816 - acc: 0.7917 - val_loss: 1.0006 - val_acc: 0.7214\n",
            "Epoch 40/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6774 - acc: 0.7929 - val_loss: 0.9968 - val_acc: 0.7222\n",
            "Epoch 41/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6734 - acc: 0.7939 - val_loss: 0.9963 - val_acc: 0.7213\n",
            "Epoch 42/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6692 - acc: 0.7954 - val_loss: 1.0102 - val_acc: 0.7202\n",
            "Epoch 43/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6653 - acc: 0.7968 - val_loss: 1.0069 - val_acc: 0.7224\n",
            "Epoch 44/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6613 - acc: 0.7975 - val_loss: 1.0164 - val_acc: 0.7188\n",
            "Epoch 45/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6571 - acc: 0.7989 - val_loss: 1.0166 - val_acc: 0.7205\n",
            "Epoch 46/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6534 - acc: 0.7998 - val_loss: 1.0224 - val_acc: 0.7202\n",
            "Epoch 47/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6496 - acc: 0.8010 - val_loss: 1.0252 - val_acc: 0.7204\n",
            "Epoch 48/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6459 - acc: 0.8022 - val_loss: 1.0286 - val_acc: 0.7187\n",
            "Epoch 49/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6423 - acc: 0.8032 - val_loss: 1.0297 - val_acc: 0.7199\n",
            "Epoch 50/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6385 - acc: 0.8045 - val_loss: 1.0387 - val_acc: 0.7205\n",
            "Epoch 51/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6352 - acc: 0.8056 - val_loss: 1.0409 - val_acc: 0.7183\n",
            "Epoch 52/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6314 - acc: 0.8065 - val_loss: 1.0543 - val_acc: 0.7171\n",
            "Epoch 53/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6283 - acc: 0.8075 - val_loss: 1.0495 - val_acc: 0.7193\n",
            "Epoch 54/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6248 - acc: 0.8083 - val_loss: 1.0550 - val_acc: 0.7175\n",
            "Epoch 55/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6216 - acc: 0.8093 - val_loss: 1.0591 - val_acc: 0.7170\n",
            "Epoch 56/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6181 - acc: 0.8105 - val_loss: 1.0617 - val_acc: 0.7167\n",
            "Epoch 57/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6155 - acc: 0.8110 - val_loss: 1.0682 - val_acc: 0.7161\n",
            "Epoch 58/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.6121 - acc: 0.8121 - val_loss: 1.0813 - val_acc: 0.7164\n",
            "Epoch 59/100\n",
            "39687/39687 [==============================] - 66s 2ms/step - loss: 0.6090 - acc: 0.8132 - val_loss: 1.0787 - val_acc: 0.7150\n",
            "Epoch 60/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6060 - acc: 0.8138 - val_loss: 1.0896 - val_acc: 0.7137\n",
            "Epoch 61/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.6030 - acc: 0.8149 - val_loss: 1.0926 - val_acc: 0.7140\n",
            "Epoch 62/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5996 - acc: 0.8160 - val_loss: 1.0979 - val_acc: 0.7116\n",
            "Epoch 63/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5969 - acc: 0.8165 - val_loss: 1.1024 - val_acc: 0.7122\n",
            "Epoch 64/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5943 - acc: 0.8175 - val_loss: 1.1093 - val_acc: 0.7109\n",
            "Epoch 65/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5915 - acc: 0.8184 - val_loss: 1.1077 - val_acc: 0.7132\n",
            "Epoch 66/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5886 - acc: 0.8191 - val_loss: 1.1135 - val_acc: 0.7130\n",
            "Epoch 67/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5858 - acc: 0.8202 - val_loss: 1.1207 - val_acc: 0.7116\n",
            "Epoch 68/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5832 - acc: 0.8205 - val_loss: 1.1275 - val_acc: 0.7110\n",
            "Epoch 69/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5807 - acc: 0.8214 - val_loss: 1.1324 - val_acc: 0.7108\n",
            "Epoch 70/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5781 - acc: 0.8223 - val_loss: 1.1386 - val_acc: 0.7095\n",
            "Epoch 71/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5759 - acc: 0.8231 - val_loss: 1.1461 - val_acc: 0.7079\n",
            "Epoch 72/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5735 - acc: 0.8234 - val_loss: 1.1469 - val_acc: 0.7109\n",
            "Epoch 73/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5709 - acc: 0.8245 - val_loss: 1.1492 - val_acc: 0.7095\n",
            "Epoch 74/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5690 - acc: 0.8251 - val_loss: 1.1448 - val_acc: 0.7096\n",
            "Epoch 75/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5665 - acc: 0.8257 - val_loss: 1.1538 - val_acc: 0.7085\n",
            "Epoch 76/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5641 - acc: 0.8261 - val_loss: 1.1590 - val_acc: 0.7083\n",
            "Epoch 77/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5619 - acc: 0.8269 - val_loss: 1.1518 - val_acc: 0.7088\n",
            "Epoch 78/100\n",
            "39687/39687 [==============================] - 67s 2ms/step - loss: 0.5600 - acc: 0.8276 - val_loss: 1.1661 - val_acc: 0.7089\n",
            "Epoch 79/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5574 - acc: 0.8282 - val_loss: 1.1680 - val_acc: 0.7067\n",
            "Epoch 80/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5556 - acc: 0.8289 - val_loss: 1.1752 - val_acc: 0.7063\n",
            "Epoch 81/100\n",
            "39687/39687 [==============================] - 68s 2ms/step - loss: 0.5540 - acc: 0.8293 - val_loss: 1.1740 - val_acc: 0.7075\n",
            "Epoch 82/100\n",
            "39687/39687 [==============================] - 69s 2ms/step - loss: 0.5516 - acc: 0.8300 - val_loss: 1.1780 - val_acc: 0.7068\n",
            "Epoch 83/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5497 - acc: 0.8306 - val_loss: 1.1963 - val_acc: 0.7051\n",
            "Epoch 84/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5479 - acc: 0.8309 - val_loss: 1.1875 - val_acc: 0.7063\n",
            "Epoch 85/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5458 - acc: 0.8315 - val_loss: 1.2005 - val_acc: 0.7050\n",
            "Epoch 86/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5439 - acc: 0.8322 - val_loss: 1.2103 - val_acc: 0.7057\n",
            "Epoch 87/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5422 - acc: 0.8327 - val_loss: 1.2023 - val_acc: 0.7055\n",
            "Epoch 88/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5407 - acc: 0.8330 - val_loss: 1.2102 - val_acc: 0.7038\n",
            "Epoch 89/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5388 - acc: 0.8334 - val_loss: 1.2022 - val_acc: 0.7048\n",
            "Epoch 90/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5371 - acc: 0.8341 - val_loss: 1.2169 - val_acc: 0.7042\n",
            "Epoch 91/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5358 - acc: 0.8345 - val_loss: 1.2165 - val_acc: 0.7039\n",
            "Epoch 92/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 0.5347 - acc: 0.8347 - val_loss: 1.2220 - val_acc: 0.7041\n",
            "Epoch 93/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5327 - acc: 0.8351 - val_loss: 1.2244 - val_acc: 0.7023\n",
            "Epoch 94/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5308 - acc: 0.8360 - val_loss: 1.2219 - val_acc: 0.7043\n",
            "Epoch 95/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5295 - acc: 0.8365 - val_loss: 1.2322 - val_acc: 0.7043\n",
            "Epoch 96/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5288 - acc: 0.8366 - val_loss: 1.2437 - val_acc: 0.7016\n",
            "Epoch 97/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 0.5275 - acc: 0.8370 - val_loss: 1.2326 - val_acc: 0.7042\n",
            "Epoch 98/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5257 - acc: 0.8373 - val_loss: 1.2389 - val_acc: 0.7023\n",
            "Epoch 99/100\n",
            "39687/39687 [==============================] - 71s 2ms/step - loss: 0.5239 - acc: 0.8379 - val_loss: 1.2442 - val_acc: 0.7034\n",
            "Epoch 100/100\n",
            "39687/39687 [==============================] - 70s 2ms/step - loss: 0.5235 - acc: 0.8382 - val_loss: 1.2436 - val_acc: 0.7029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QqZL7WRkyjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "092f7e67-04c2-4ac5-a1fc-4709eef1c8d2"
      },
      "source": [
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['_BOM_']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_input_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_EOM_' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: به نام خداوند جان و خرد \n",
            "Decoded sentence: نماند ان زمان روزگاران مرد_EOM_\n",
            "-\n",
            "Input sentence: خداوند نام و خداوند جای \n",
            "Decoded sentence: همان نامداری و فرخ سپای_EOM_\n",
            "-\n",
            "Input sentence: خداوند کیوان و گردان سپهر \n",
            "Decoded sentence: بید انچش دیو ازاده ست_EOM_\n",
            "-\n",
            "Input sentence: ز نام و نشان و گمان برترست \n",
            "Decoded sentence: و گر بر نهان کار او راه بست_EOM_\n",
            "-\n",
            "Input sentence: به بینندگان افریننده را \n",
            "Decoded sentence: بد از تن سر از تن به یک روز را_EOM_\n",
            "-\n",
            "Input sentence: نیابد بدو نیز اندیشه راه \n",
            "Decoded sentence: نه سر بر سر افگند بر کار راه_EOM_\n",
            "-\n",
            "Input sentence: سخن هر چه زین گوهران بگذرد \n",
            "Decoded sentence: ندید از برش روزگاران کرد_EOM_\n",
            "-\n",
            "Input sentence: خرد گر سخن برگزیند همی \n",
            "Decoded sentence: منان دان به اندیشه از امدش_EOM_\n",
            "-\n",
            "Input sentence: ستودن نداند کس او را چو هست \n",
            "Decoded sentence: سخن گشت با درد و سنگ امدست_EOM_\n",
            "-\n",
            "Input sentence: خرد را و جان را همی سنجد اوی \n",
            "Decoded sentence: بران نامور شهریار از جوی_EOM_\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}